{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transparent_image(background, foreground, x_offset=None, y_offset=None):\n",
    "    bg_h, bg_w, bg_channels = background.shape\n",
    "    fg_h, fg_w, fg_channels = foreground.shape\n",
    "\n",
    "    assert bg_channels == 3, f'background image should have exactly 3 channels (RGB). found:{bg_channels}'\n",
    "    assert fg_channels == 4, f'foreground image should have exactly 4 channels (RGBA). found:{fg_channels}'\n",
    "\n",
    "    # center by default\n",
    "    if x_offset is None: x_offset = (bg_w - fg_w) // 2\n",
    "    if y_offset is None: y_offset = (bg_h - fg_h) // 2\n",
    "\n",
    "    w = min(fg_w, bg_w, fg_w + x_offset, bg_w - x_offset)\n",
    "    h = min(fg_h, bg_h, fg_h + y_offset, bg_h - y_offset)\n",
    "\n",
    "    if w < 1 or h < 1: return\n",
    "\n",
    "    # clip foreground and background images to the overlapping regions\n",
    "    bg_x = max(0, x_offset)\n",
    "    bg_y = max(0, y_offset)\n",
    "    fg_x = max(0, x_offset * -1)\n",
    "    fg_y = max(0, y_offset * -1)\n",
    "    foreground = foreground[fg_y:fg_y + h, fg_x:fg_x + w]\n",
    "    background_subsection = background[bg_y:bg_y + h, bg_x:bg_x + w]\n",
    "\n",
    "    # separate alpha and color channels from the foreground image\n",
    "    foreground_colors = foreground[:, :, :3]\n",
    "    alpha_channel = foreground[:, :, 3] / 255  # 0-255 => 0.0-1.0\n",
    "\n",
    "    # construct an alpha_mask that matches the image shape\n",
    "    alpha_mask = np.dstack((alpha_channel, alpha_channel, alpha_channel))\n",
    "\n",
    "    # combine the background with the overlay image weighted by alpha\n",
    "    composite = background_subsection * (1 - alpha_mask) + foreground_colors * alpha_mask\n",
    "\n",
    "    # overwrite the section of the background image that has been updated\n",
    "    background[bg_y:bg_y + h, bg_x:bg_x + w] = composite\n",
    "\n",
    "    return background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_frame(frame, bg, transparency=220, color=np.float32([220,50,50]), kernel=7):\n",
    "    frame = np.transpose(frame, axes=[1, 0, 2])\n",
    "    frame = np.flip(frame, axis=0)\n",
    "    bg = cv2.resize(bg, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "    frame *= 255\n",
    "    frame[:,:,3] = np.mean(frame[:,:,:3], axis=2)\n",
    "    frame[np.where(frame[:,:,3] > 0)] = transparency\n",
    "    frame[:,:,:3] = color\n",
    "    frame = cv2.morphologyEx(frame, cv2.MORPH_CLOSE, np.ones((kernel,kernel)))\n",
    "\n",
    "    add_transparent_image(bg, frame)\n",
    "\n",
    "    return bg\n",
    "\n",
    "def make_dataset_video(video, **kwargs):\n",
    "    files = os.listdir('backgrounds/')\n",
    "    bg = None \n",
    "    while bg is None:\n",
    "        bg = cv2.imread(f'backgrounds/{random.choice(files)}')\n",
    "\n",
    "    new_video = np.zeros((*video.shape[:3], 3), dtype=np.uint8)\n",
    "    p_color = np.uint8([random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)])\n",
    "    transparency = random.randint(127, 255)\n",
    "\n",
    "    for i in range(video.shape[0]):\n",
    "        new_video[i] = make_dataset_frame(\n",
    "            video[i].copy(), \n",
    "            bg.copy(), \n",
    "            transparency=transparency, \n",
    "            color=p_color,\n",
    "              **kwargs\n",
    "        )\n",
    "\n",
    "    return new_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot2vid import PlotRecorder\n",
    "\n",
    "def get_sample(material, name, time_in_seconds=4):\n",
    "    center = [0.1 + 0.8 * random.random(), 0.1 + 0.8 * random.random()]\n",
    "    max_radius = min(center[0], center[1], 1 - center[0], 1 - center[1])\n",
    "    radius = min(0.1 + 0.5 * random.random(), max_radius)\n",
    "\n",
    "    p, v = generate_granular_trajectory(\n",
    "        material, \n",
    "        time_in_seconds=time_in_seconds, \n",
    "        video_resolution=64, \n",
    "        sim_resolution=12, \n",
    "        center=center,\n",
    "        radius=radius\n",
    "    )\n",
    "    vid = make_dataset_video(v, kernel=5)\n",
    "\n",
    "    recorder = PlotRecorder(f'{name}.mp4', fps=15)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8,4), dpi=100)\n",
    "\n",
    "    for i in tqdm(range(vid.shape[0])):\n",
    "        if i % 5 == 0:\n",
    "            axs[0].imshow(vid[i])\n",
    "            axs[1].clear()\n",
    "            axs[1].scatter(p[i,:,0], p[i,:,1])\n",
    "            axs[1].axis('equal')\n",
    "            axs[1].set_xlim(0,1.0)\n",
    "            axs[1].set_ylim(0,1)\n",
    "\n",
    "            recorder.add(fig)\n",
    "\n",
    "    recorder.close()\n",
    "\n",
    "    return p, vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts, frames = get_sample('WATER', f'datasets/fluids/videos/water_test_{traj_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_PER_CLASS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj_number in range(SAMPLES_PER_CLASS):\n",
    "    parts, frames = get_sample('WATER', f'datasets/fluids/videos/water_test_{traj_number}')\n",
    "    torch.save(parts, f'datasets/fluids/particles/water_test_{traj_number}')\n",
    "    torch.save(frames, f'datasets/fluids/frames/water_test_{traj_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj_number in range(SAMPLES_PER_CLASS):\n",
    "    parts, frames = get_sample('SAND', f'datasets/fluids/videos/sand_test_{traj_number}')\n",
    "    torch.save(parts, f'datasets/fluids/particles/sand_test_{traj_number}')\n",
    "    torch.save(frames, f'datasets/fluids/frames/sand_test_{traj_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj_number in range(SAMPLES_PER_CLASS):\n",
    "    parts, frames = get_sample('SNOW', f'datasets/fluids/videos/snow_test_{traj_number}')\n",
    "    torch.save(parts, f'datasets/fluids/particles/snow_test_{traj_number}')\n",
    "    torch.save(frames, f'datasets/fluids/frames/snow_test_{traj_number}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj_number in range(SAMPLES_PER_CLASS):\n",
    "    parts, frames = get_sample('ELASTIC', f'datasets/fluids/videos/elastic_test_{traj_number}')\n",
    "    torch.save(parts, f'datasets/fluids/particles/elastic_test_{traj_number}')\n",
    "    torch.save(frames, f'datasets/fluids/frames/elastic_test_{traj_number}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
