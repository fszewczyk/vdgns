{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.gn as gn \n",
    "import utils.generator as gen\n",
    "import utils.visualization as viz\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from plot2vid import PlotRecorder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/fluids'\n",
    "\n",
    "water_particles, water_frames = [], []\n",
    "elastic_particles, elastic_frames = [], []\n",
    "snow_particles, snow_frames = [], []\n",
    "sand_particles, sand_frames = [], []\n",
    "\n",
    "for sample in tqdm(range(30)):\n",
    "    water_particles.append(torch.load(f'{path}/particles/water_{sample}'))\n",
    "    elastic_particles.append(torch.load(f'{path}/particles/elastic_{sample}'))\n",
    "    snow_particles.append(torch.load(f'{path}/particles/snow_{sample}'))\n",
    "    sand_particles.append(torch.load(f'{path}/particles/sand_{sample}'))\n",
    "\n",
    "    water_frames.append(torch.load(f'{path}/frames/water_{sample}'))\n",
    "    elastic_frames.append(torch.load(f'{path}/frames/elastic_{sample}'))\n",
    "    snow_frames.append(torch.load(f'{path}/frames/snow_{sample}'))\n",
    "    sand_frames.append(torch.load(f'{path}/frames/sand_{sample}'))\n",
    "\n",
    "all_particles = [*water_particles, *elastic_particles, *snow_particles, *sand_particles]\n",
    "all_frames = [*water_frames, *elastic_frames, *snow_frames, *sand_frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trajectories = len(all_particles)\n",
    "trajectory_length = water_particles[0].shape[0]\n",
    "video_shape = all_frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_trajectories = all_particles\n",
    "raw_videos = torch.zeros((len(all_frames), *video_shape), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, frame in enumerate(all_frames):\n",
    "    raw_videos[i] = torch.tensor(frame, dtype=torch.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, particles in enumerate(all_particles):\n",
    "    particle_trajectories[i] = torch.tensor(particles, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP = 0.01\n",
    "MAX_FPS = 100\n",
    "RESOLUTION = 64\n",
    "\n",
    "PHYSICAL_HIDDEN_ENCODING_SIZE = 32\n",
    "PHYSICAL_ENCODING_SIZE = 4\n",
    "\n",
    "EDGE_EMBEDDING_SIZE = 48\n",
    "NODE_EMBEDDING_SIZE = 48\n",
    "NUM_GN_LAYERS = 3\n",
    "GN_PROCESSOR_DEPTH = 1\n",
    "PAST_VELOCITIES = 3\n",
    "GRAPH_NETWORK_EPOCHS = 2\n",
    "CONNECTIVITY_RADIUS=0.12\n",
    "EDGE_NOISE_STD = 0.05\n",
    "PARTICLE_NOISE_STD = 0.002\n",
    "MIN_STEP = 50\n",
    "LIMIT_EDGES_PER_PARTICLE = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = raw_videos[:,::(MAX_FPS // 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset = gn.ParticleVideoDataset(\n",
    "    number_of_classes=4,\n",
    "    raw_particles=particle_trajectories, \n",
    "    videos=videos,\n",
    "    video_encoder=None,\n",
    "    system='fluid',\n",
    "    past_velocities=PAST_VELOCITIES,\n",
    "    connectivity_radius=CONNECTIVITY_RADIUS,\n",
    "    edge_noise_std=EDGE_NOISE_STD,\n",
    "    particle_noise_std=PARTICLE_NOISE_STD,\n",
    "    minimum_rollout_step=MIN_STEP,\n",
    "    limit_edges_per_particle=LIMIT_EDGES_PER_PARTICLE\n",
    ")\n",
    "\n",
    "torch.save(graph_dataset, 'datasets/fluids/classes=4_samples=30.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset.one_hot_encode_class = False\n",
    "graph_dataset.minimum_rollout_step = MIN_STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import time\n",
    "\n",
    "def train(model: nn.Module,\n",
    "          train_loader,\n",
    "          validation_loader=None,\n",
    "          n_epochs=10,\n",
    "          lr=0.001,\n",
    "          lr_decay=1.0,\n",
    "          verbose=False,\n",
    "          additional_models=[],\n",
    "          checkpoint_name=None):\n",
    "    stats = {\n",
    "        'loss_iteration': [],\n",
    "        'loss_epoch': [],\n",
    "        'loss_validation': []\n",
    "    }\n",
    "\n",
    "    # Training setup\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    optimizers = [torch.optim.Adam(model.parameters(), lr=lr)]\n",
    "    for m in additional_models:\n",
    "        optimizers.append(torch.optim.Adam(m.parameters(), lr=lr))\n",
    "    \n",
    "    schedulers = []\n",
    "    for o in optimizers:\n",
    "        schedulers.append(torch.optim.lr_scheduler.ExponentialLR(o, gamma=lr_decay))\n",
    "\n",
    "    # Actual training\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss = 0.0\n",
    "        graph_dataset.edge_noise_std = EDGE_NOISE_STD\n",
    "        graph_dataset.particle_noise_std = PARTICLE_NOISE_STD\n",
    "        \n",
    "        for i, graph in tqdm(enumerate(train_loader)):\n",
    "            for o in optimizers:\n",
    "                o.zero_grad()\n",
    "\n",
    "            predicted_graph = model(graph.clone())\n",
    "\n",
    "            loss = criterion(predicted_graph.x, graph.y)\n",
    "            loss.backward()\n",
    "\n",
    "            for o in optimizers:\n",
    "                o.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            stats['loss_iteration'].append(loss.item())\n",
    "            \n",
    "            if verbose and i > 0 and i % 500 == 0:\n",
    "                print(f\"Epoch: {epoch} Batch: {i} \\t Loss: {train_loss / i}\")\n",
    "        \n",
    "        for s in schedulers:\n",
    "            s.step()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        stats['loss_epoch'].append(train_loss)\n",
    "        if verbose:\n",
    "            print(f\"Epoch: {epoch} \\t Loss: {train_loss}\")\n",
    "            \n",
    "        if validation_loader is not None:\n",
    "            graph_dataset.edge_noise_std = 0.0\n",
    "            graph_dataset.particle_noise_std = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                validation_loss = []\n",
    "                for i, graph in enumerate(validation_loader):\n",
    "                    predicted_graph = model(graph)\n",
    "                    loss = criterion(predicted_graph.x, graph.y)\n",
    "                    validation_loss.append(loss.item())\n",
    "\n",
    "                stats['loss_validation'].append(sum(validation_loss) / len(validation_loss))\n",
    "                \n",
    "                if verbose:\n",
    "                    print(f'\\tValidation loss: {stats[\"loss_validation\"][-1]}')\n",
    "\n",
    "        if checkpoint_name:\n",
    "            torch.save(model, f'models/fluids/experiments/model_{checkpoint_name}_epoch={epoch}.torch')\n",
    "            for i, m in enumerate(additional_models):\n",
    "                torch.save(m, f'models/fluids/experiments/additional_{i}_{checkpoint_name}_epoch={epoch}.torch')\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.video import ConvVideoEncoder\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from utils.builder import build_mlp\n",
    "\n",
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 frame_encoder: nn.Module,\n",
    "                 latent_frame_size: int,\n",
    "                 encoding_size: int,\n",
    "                 hidden_state_size=32,\n",
    "                 num_lstm_layers=1):\n",
    "        super(VideoEncoder, self).__init__()\n",
    "\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "\n",
    "        self.frame_encoder = frame_encoder\n",
    "\n",
    "        self.linear_in = nn.Linear(latent_frame_size, hidden_state_size)\n",
    "        self.lstm = nn.LSTM(hidden_state_size, hidden_state_size, num_layers=num_lstm_layers, batch_first=True)\n",
    "        self.linear_out = nn.Linear(hidden_state_size, encoding_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, height, width, channels = x.shape\n",
    "\n",
    "        x = self.frame_encoder(x.view(batch_size * seq_length, height, width, channels)).view(batch_size, seq_length, -1)\n",
    "\n",
    "        x = self.linear_in(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.linear_out(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "frame_encoder = nn.Sequential(\n",
    "    nn.Flatten(), \n",
    "    build_mlp(\n",
    "        input_dim = RESOLUTION * RESOLUTION * 3,\n",
    "        hidden_layers = [8 * RESOLUTION, 2 * RESOLUTION],\n",
    "        out_dim = 128,\n",
    "        activations=['relu', 'relu', 'relu']\n",
    "    )\n",
    ")\n",
    "\n",
    "video_encoder = torch.load('models/fluids/experiments/additional_0_fps=20_classes=4_epoch=17.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset.video_encoder = video_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_encoder(videos[:5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset_loader = DataLoader(graph_dataset, batch_size=8, shuffle=True)\n",
    "validation_graph_dataset_loader = DataLoader(graph_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_data = {\n",
    "    'node_mean': graph_dataset.node_mean.detach().clone(),\n",
    "    'node_std': graph_dataset.node_std.detach().clone(),\n",
    "    'edge_mean': graph_dataset.edge_mean.detach().clone(),\n",
    "    'edge_std': graph_dataset.edge_std.detach().clone()\n",
    "}\n",
    "        \n",
    "postprocessing_data = {\n",
    "    'out_mean': graph_dataset.out_mean.detach().clone(),\n",
    "    'out_std': graph_dataset.out_std.detach().clone(),\n",
    "}\n",
    "        \n",
    "graph_network = gn.build_encoder_processor_decoder(\n",
    "    node_size=PAST_VELOCITIES * 2 + PHYSICAL_ENCODING_SIZE + 4,\n",
    "    edge_size=3, \n",
    "    node_latent=NODE_EMBEDDING_SIZE, \n",
    "    edge_latent=EDGE_EMBEDDING_SIZE,\n",
    "    output_size=2,\n",
    "    num_gn_layers=NUM_GN_LAYERS,\n",
    "    shared_gn_layers=False,\n",
    "    processor_depth=GN_PROCESSOR_DEPTH,\n",
    "    aggregation_fun='sum',\n",
    "    preprocessing_data=preprocessing_data,\n",
    "    postprocessing_data=postprocessing_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset.edge_noise_std = EDGE_NOISE_STD / 5\n",
    "graph_dataset.particle_noise_std = PARTICLE_NOISE_STD / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training the graph network')\n",
    "graph_network_stats = train(\n",
    "    model=graph_network,\n",
    "    train_loader=graph_dataset_loader,\n",
    "    validation_loader=validation_graph_dataset_loader,\n",
    "    n_epochs=100,\n",
    "    lr=0.0001,\n",
    "    lr_decay=0.95,\n",
    "    verbose=True,\n",
    "    additional_models=[video_encoder],\n",
    "    checkpoint_name=f'_fps={20}_classes={4}',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
